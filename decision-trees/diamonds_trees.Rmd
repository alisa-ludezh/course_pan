---
title: "Diamond Pricing Using Decision Tree Algorithms"
author: "Gleb Zakhodyakin"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=F,warning=F}
library(tidyverse)
library(mlr) #to get model performance metrics

# A few decision tree algorithms:
library(rpart) #CART
library(party) # Conditional inference trees
library(randomForest) # Random Forest

# Tree visualization:
library(rpart.plot) # for rpart
library(partykit) # for party & C5.0
```


# Introduction

![Diamonds](diamonds.jpg)

Diamond's price is affected by multiple factors. The most important factors are considered 'the 4 C's': **carate** (weight), **color**,  **clarity** and **cut**. 

The data for building the model are available in the `diamonds_train.txt` file.

**Variables:**

- **carat** - diamond's weight in carat (1 carat = 0.2 g)

- **colour** - diamond's color shade (D - I). The finest diamonds have no visible shade and are completely transparent (D). The least valuable diamonds have a yellowish shade (I)

- **clarity** - most diamonds have small defects visible on the surface or inside the stone. The clarity rating tells how noticeable these defects are. The best stones have IF (Internally Flawless) rating. These have no visible inclusions, but may have small scratches on the surface noticeable only to experts. The VVS1 and VVS2 (Very Very Slightly Included) diamonds have smallest, barely noticeable inclusions. The VS1 and VS2 diamonds have some small inclusions, clearly noticeable under 10x magnification. 

- **certification**  - most diamonds are certified by an independent agency before sale. This certification ensures buyer that the specs provided by the seller are true. There are 3 international agencies issuing these certificates - GIA, IGI and HRD 

- **price** - diamond price, in Singapore Dollars (prices are quite outdated, they originate from around ~2000). This is the target variable you need to predict.


All diamonds have specific cut to emphasize the natural beauty of each stone and to mask the defects. The optimal cut shouldn't be too tall or too flat, since this will impair the play of light. There are several cut types, however this file contains only the diamonds of the classical, round cut.


You can learn more about diamond pricing here:
- [Lumera Diamonds Education](http://www.lumeradiamonds.com/diamond-education/index)
- [Diamond Prices](https://www.diamonds.pro/diamond-prices/)
- [James Allen Diamonds Gallery](http://www.jamesallen.com/loose-diamonds/all-diamonds/)

# Loading the data



Let's load the data. Some algorithms can't handle character data, so all text variables must be converted into factors.

```{r}
d <- read_tsv('diamonds_train.txt', skip = 34) %>%
  mutate(colour = factor(colour),
         clarity = factor(clarity, 
                          levels = c('VS2','VS1','VVS2','VVS1', 'IF')),
         certification = factor(certification))
head(d)
```



Since tree-based models are prone to overfitting the training data, let's split the dataset into the training and test samples:

```{r}
set.seed(555)
selected_rows <- sample(x = nrow(d), size = 0.8 * nrow(d))

d_train <- d[selected_rows, ]
d_test <- d[-selected_rows, ]

```


# Building the models

## CART algorithm (rpart)


```{r}
m_rpart <- rpart(price ~ ., data = d_train)
rpart.plot(m_rpart)
```

```{r}
summary(m_rpart)
```


Let's predict the prices and compare performance on the training and test data:

```{r}
# Predictions for the training data:
fit_rpart <- predict(m_rpart, newdata = d_train, type = "vector")

# Predictions for the test data:
pred_rpart <- predict(m_rpart, newdata = d_test, type = "vector")
```

Compute the mean absolute percent error:

 - for the training sample:
```{r}

measureMAPE(truth = d_train$price, response = fit_rpart) * 100

```

 - for the test sample:
```{r}
measureMAPE(truth = d_test$price, response = pred_rpart) * 100
```

```{r}
qplot(x = fit_rpart, y = d_train$price, geom = 'point') + 
  geom_line(aes(fit_rpart, fit_rpart),
            color = 'red',
            linetype = 'dashed') +
  labs(title = 'Prediction from CART (rpart)',
       x = 'Predicted Price',
       y = 'Actual Price')

```





## CART (tuned model)

As we can see, the predictions are too crude. The tree outputs just 6 possible prices. This is the effect of over-simplification. Let's play with the algorithm parameters responsible for the tree complexity.


```{r}
m_rpart_tuned <- rpart(price ~ ., 
                       data = d_train,
                       control = 
                         rpart.control(
                            minsplit = 5, # minimum node size for splitting
                            cp = 0.001, # minimum relative improvement for splitting
                            maxdepth = 20 # maximum depth for the tree
                       ))
rpart.plot(m_rpart_tuned)
```
```{r}
plotcp(m_rpart_tuned)
```


```{r}
summary(m_rpart_tuned)
```


Let's predict the prices and compare performance on the training and test data:

```{r}
# Predictions for the training data:
fit_rpart_tuned <- predict(m_rpart_tuned, newdata = d_train, type = "vector")

# Predictions for the test data:
pred_rpart_tuned <- predict(m_rpart_tuned, newdata = d_test, type = "vector")
```

Compute the mean absolute percent error:

 - for the training sample:
```{r}

measureMAPE(truth = d_train$price, response = fit_rpart_tuned) * 100

```

 - for the test sample:
```{r}
measureMAPE(truth = d_test$price, response = pred_rpart_tuned) * 100
```

```{r}
qplot(x = fit_rpart_tuned, y = d_train$price, geom = 'point') + 
  geom_line(aes(fit_rpart_tuned, fit_rpart_tuned),
            color = 'red',
            linetype = 'dashed') +
  labs(title = 'Prediction from tuned CART (rpart)',
       x = 'Predicted Price',
       y = 'Actual Price')

```

Prediction quality has improved.



## Conditional Inference Tree Algorithm (ctree)

```{r, fig.width = 20, fig.height = 10}

m_ctree <- ctree(price ~ ., data = d_train)
plot(m_ctree)
```


Let's predict the prices and compare performance on the training and test data:

```{r}
# Predictions for the training data:
fit_ctree <- predict(m_ctree, newdata = d_train)

# Predictions for the test data:
pred_ctree <- predict(m_ctree, newdata = d_test)

```

Compute the mean absolute percent error:

 - for the training sample:
```{r}

measureMAPE(truth = d_train$price, response = fit_ctree) * 100

```

 - for the test sample:
```{r}
measureMAPE(truth = d_test$price, response = pred_ctree) * 100
```

```{r}
qplot(x = fit_ctree, y = d_train$price, geom = 'point') + 
  geom_line(aes(fit_ctree, fit_ctree),
            color = 'red',
            linetype = 'dashed') +
  labs(title = 'Prediction from Conditional Inference Tree (ctree)',
       x = 'Predicted Price',
       y = 'Actual Price')

```



## Random Forest Algorithm (randomForest)

Let's try an ensemble algorithm - [Random Forest](https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/).


```{r}
m_rf <- randomForest(price ~ ., data = d_train, 
                     ntree = 101, #number of trees in the ensemble
                     maxnodes = 60, #maximum size of a tree
                     mtry = 4, # number of variables to try at each split, was too low
                     importance = TRUE#assess variable importance
                     )
print(m_rf)
```


```{r}
varImpPlot(m_rf, main = 'Variable Importance Plot')
```


Let's predict the prices and compare performance on the training and test data:

```{r}
# Predictions for the training data:
fit_rf <- predict(m_rf, newdata = d_train)

# Predictions for the test data:
pred_rf <- predict(m_rf, newdata = d_test)
```

Compute the mean absolute percent error:

 - for the training sample:
```{r}
measureMAPE(truth = d_train$price, response = fit_rf) * 100
```

 - for the test sample:
```{r}
measureMAPE(truth = d_test$price, response = pred_rf) * 100
```

```{r}
qplot(x = fit_rf, y = d_train$price, geom = 'point') + 
  geom_line(aes(fit_rf, fit_rf),
            color = 'red',
            linetype = 'dashed') +
  labs(title = 'Prediction from Random Forest',
       x = 'Predicted Price',
       y = 'Actual Price')

```


# Forecasting the prices

Re-building the models using complete training data:

```{r}
m_rpart_tuned_full <- rpart(price ~ ., 
                       data = d,
                       control = 
                         rpart.control(
                            minsplit = 5, # minimum node size for splitting
                            cp = 0.001, # minimum relative improvement for splitting
                            maxdepth = 20 # maximum depth for the tree
                       ))

m_ctree_full <- ctree(price ~ ., data = d)

m_rf_full <- randomForest(price ~ ., data = d, 
                     ntree = 101, #number of trees in the ensemble
                     maxnodes = 60, #maximum size of a tree
                     mtry = 4, # number of variables to try at each split, was too low
                     importance = TRUE#assess variable importance
                     )

```

Load the test data:

```{r}

dnew <- read_tsv('diamonds_test.txt') %>%
  mutate(colour = factor(colour),
         clarity = factor(clarity, 
                          levels = c('VS2','VS1','VVS2','VVS1', 'IF')),
         certification = factor(certification))

head(dnew)
```

Compute and save the forecasts 

```{r}
# Tuned rpart
m_rpart_tuned_full %>%
  predict(newdata = dnew) %>%
  tibble(price = .) %>%
  write_csv(path = 'prices_rpart.txt')

# ctree
m_ctree %>%
  predict(newdata = dnew) %>%
  tibble(price = .) %>%
  write_csv(path = 'prices_ctree.txt')

# Random Forest
m_rf_full %>%
  predict(newdata = dnew) %>%
  tibble(price = .) %>%
  write_csv(path = 'prices_randomForest.txt')

```

