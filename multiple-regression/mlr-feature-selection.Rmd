---
title: "Множественная регрессия: отбор факторов и диагностика модели"
author: "Заходякин Г.В., postlogist@gmail.com"
date: "21.02.2017"
output: 
  html_document: 
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
options(width = 85) # ширина текстового вывода
options(digits = 3) # число знаков после запятой в выводе 
```


# Введение

В модель множественной регрессии можно включать несколько объясняющих переменных. Вопрос в том, какие именно переменные лучше всего включить. В этом блокноте мы рассмотрим на примере прогнозирования топливной эффективности машин несколько подходов к отбору факторов в модель. Также здесь рассмотрены вопросы определения наиболее важных предикторов среди включенных в модель и преобразования данных для облегчения интерпретации коэффициентов модели. В заключение будет затронута проблема диагностики остатков модели и улучшения ее качества.

# Подготовка

## Загрузка пакетов

```{r Загрузка пакетов, message=FALSE}
library(readr) # считывание данных из текстовых файлов
library(memisc) # удобное сравнение моделей в таблице
library(tidyverse) # манипулирование данными 
library(ggplot2) # визуализация 
library(ggfortify) # визуализация диагностических графиков
library(modelr) # вспомогательные функции для работы с моделями
library(broom) # преобразование результатов моделирования в табличный вид
library(GGally) # построение матрицы диаграмм рассеяния 
library(car) # функции для степенных преобразований
library(forcats) # работа с факторами
library(glmnet) # Лассо и ридж-регрессия
```


## Загрузка данных

```{r Загрузка данных, warning = FALSE, message = FALSE}
cars <- read_csv2("datasets/cars.csv", skip = 25) 
```

Разведочный анализ выполнялся в [первой части примера](mlr-modeling.Rmd).
Здесь мы лишь исключим 5 наблюдений с пропущенными данными.

```{r Удаление строк с пропусками}
cars <- na.omit(cars)
```


# Проблема мультиколлинеарности

## Причина мультиколлинеарности
Рассмотрим матрицу диаграмм рассеяния для различных характеристик машин.

```{r Матрица диаграмм рассеяния, fig.height=10, fig.width=10}

# Матрица диаграмм рассеяния
cars %>% 
  select_if(is.numeric) %>%
  ggpairs(lower = list(continuous = wrap("smooth_lm", color = 'blue')))
```

Все количественные переменные коррелированы с `mpg`. Но можно видеть, что многие из них коррелированы и между собой.
Например, коэффициент корреляции для длины машины и ее колесной базы равен 0.84. Эти переменные передают сходную информацию о машине:

![Колесная база](figures/wheelbase.png)


Термин **"мультиколлинеарность"** (*multicollinearity*) обозначает ситуацию, в которой несколько объясняющих переменных, включенных в модель, коррелированы между собой, т.е. несут дублирующуюся информацию. Мультиколлинеарность - это сильная линейная связь между двумя и более переменными в модели.


## Как проявляется мультиколлинеарность

Рассмотрим на примере, как мультиколлинеарность влияет на результаты моделирования. Для этого добавим к построенной ранее модели с весом и мощностью машины еще одну переменную - `fuel_cap` (емкость топливного бака). Сама по себе эта переменная - хороший предиктор для `mpg` и имеет второй по абсолютной величине коэффициент корреляции с `mpg`: $r = -0.80$. Однако эта переменная сильно коррелирована также и с весом машины: $r = 0.86$.

Изменения в модели удобно отразить в таблице, полученной с помощью функции `memisc::mtable()`.

```{r Изменения в модели при добавлении коррелированного с другими предиктора}
m_wgtpow <- lm(mpg ~ curb_wgt + horsepow, data = cars)
m_wgtpowfuel <- lm(mpg ~ curb_wgt + horsepow + fuel_cap, data = cars)

memisc::mtable(m_wgtpow, m_wgtpowfuel)
```

После добавления еще одной переменной качество модели практически не изменилось: на 0.1 снизилась стандартная ошибка, а коэффициент детерминации остался прежним (с точностью до одного десятичного знака).

Но обратим внимание на то, что стандартная ошибка для коэффициента `curb_wgt` увеличилась почти вдвое. Это означает, что погрешность оценки этого коэффициента увеличилась. Это - одно из негативных проявлений мультиколлинеарности.

Помимо увеличения стандартных ошибок для угловых коэффициентов, мультиколлинеарность затрудняет интерпретацию отдельных коэффициентов. Ранее мы предполагали, что вклад каждого предиктора независим от других и интерпретировали угловой коэффициент $b_j$ как предельный эффект для предиктора $x_j$, т.е. как изменение $y$ при единичном изменении $x_j$ и зафиксированных значениях остальных предикторов. Но поскольку теперь в модели есть взаимосвязанные предикторы, например вес машины и размер топливного бака, больше невозможно изменять эти величины по отдельности. Для более тяжелой машины нужен больший топливный бак, потому что ей нужно больше топлива.

В теории множественной регрессии доказывается, что оценки коэффициентов модели взаимосвязаны между собой. Связь между ними характеризуется **ковариационной матрицей** модели  (*variance-covariance matrix*), которая получается как один из побочных результатов применения метода наименьших квадратов. Эта матрица содержит на главной диагонали дисперсии оценок коэффициентов (т.е. квадраты их стандартных ошибок), а для недиагональных элементов - ковариации оценок соответствующих коэффициентов модели.

```{r Ковариационная матрица}
round(vcov(m_wgtpowfuel), 3) 
```

Если два коэффициента имеют большую ковариацию, то стандартная ошибка для коэффициентов увеличится. Также, если один из предикторов исключить из модели, то изменится значение коэффициента для оставшегося предиктора. В некоторых случаях может поменяться даже направленность связи (знак коэффициента). Например, может оказаться, что в модели предсказания спроса цена будет иметь положительный угловой коэффициент, что невозможно с точки зрения здравого смысла.


## Как измерить степень проявления мультиколлинеарности

Для измерения степени выраженности мультиколлинеарности, используется показатель **коэффициента роста дисперсии** (*variance inflation factor*), который вычисляется для каждого предиктора в модели:

$$ VIF_j = \frac{1}{1-R^2_j}, j=1 \ldots k $$

В этой формуле $R^2_j$ - коэффициент детерминации регрессии предиктора $j$ на остальные $k-1$ предикторов. Этот коэффициент показывает, насколько остальные предикторы  повторяют информацию, содержащуюся в предикторе $j$. Для модели с $k=2$ предикторами, $R^2_j$ - это квадрат коэффициента корреляции между этими предикторами.

Если предиктор $j$ не дублирует информацию из других переменных, т.е. не связан с ними линейными зависимостями, то  $R^2_j=0$ и $VIF_j=1$. Если такие зависимости есть, то $VIF_j > 1$. 

Близкое к 1 значение $VIF_j$ позволяет заключить, что мультиколлинеарность при добавлении предиктора $j$ в модель отсуствует или незначительна. Этот предиктор не меняет стандартные ошибки и значения других коэффициентов в модели.

Если значение $VIF_j \gg 1$, то оценка углового для этого коэффициента нестабильна. При изменении набора предикторов коэффициент может сильно измениться, или стать незначимым. Большое значение $VIF$ означает дублирование информации в различных предикторах. Информация, добавленная с переменной, у которой большой $VIF$, уже была объяснена другими переменными в модели.

При $VIF > 4$ можно предположить выраженную мультиколлинеарность.

Для вычисления коэффициентов роста дисперсии в R используется функция `vif()`:

```{r Вычисление коэффициентов роста дисперсии}
vif(m_wgtpowfuel) %>% 
  round(1)
```

Хотя мультиколлинеарность и создает проблемы для статистического вывода (оценки значимости коэффициентов) и интерпретации угловых коэффициентов, она никак не мешает прогнозированию по данной модели. Проблема при прогнозировании возникает только если коэффициентов в модели очень много и для оценки каждого из них приходится всего несколько наблюдений (т.е. отношение числа наблюдений к числу коэффициентов мало). В этом случае модель может переобучиться - запомнить случайный шум, присутствующий в исходных данных вместо закономерных связей. Переобученная модель дает плохой результат при прогнозировании на новых данных.


## Линейная зависимость предикторов

В случае, когда мультиколлинеарность является строгой, т.е. несколько объясняющих переменных в модели связаны совершенной линейной зависимостью, коэффициенты модели невозможно оценить. В формуле для вычисления коэффициентов используется операция обращения матрицы данных, которая может быть выполнена только при условии линейной независимости столбцов.

Вычислим ширину машины в сантиметрах на основе ширины в дюймах и попытаемся оценить модель, в которую включены эти два предиктора. Очевидно, что один из них может быть вычислен на основе другого с помощью умножения на константу, т.е. они линейно зависимы.

```{r Попытка оценить модель с линейно зависимыми предикторами}

cars_multi <- cars %>%
  mutate(width_cm = width * 2.54)

m_width_cm <- lm(mpg ~ width_cm, data = cars_multi)
coef(m_width_cm) # нет проблем

m_width_both <- lm(mpg ~ width + width_cm, data = cars_multi)
coef(m_width_both) # одна из линейно зависимых переменных автоматически исключена
```

## Линейная зависимость между фиктивными переменными

Частным случаем линейной зависимости между предикторами является линейная зависимость между фиктивными переменными (**dummy trap**).
Линейная зависимость между фиктивными переменными возникает, если включить в модель фиктивную переменную для каждой группы. Например, ранее мы строили модель зависимости пробега от мощности с учетом типа машины. Чтобы учесть тип, который является дискретной переменной (грузовик или легковой автомобиль), необходимо преобразовать тип в фиктивные переменные. Мы создали переменную `truck`, которая принимает значение 1 для грузовиков и 0 для автомобилей. Если же включить в модель  еще одну переменную - `automobile`, которая принимает значение 1 уже для легковых машин, то переменные окажутся линейно зависимыми. Машина может быть либо грузовиком, либо легковым автомобилем, поэтому справедливо равенство:

$$automobile + truck = 1$$

Из этого следует, что зная одну из переменных можно всегда определить значение второй, т.е. они линейно зависимы.


```{r Линейная зависимость для фиктивных переменных}
cars_dummy <- cars %>%
  mutate(truck = ifelse(type == 'Truck', 1, 0), 
         automobile = ifelse(type == 'Automobile', 1, 0))

m_dummy <- lm(mpg ~ horsepow + truck + automobile, data = cars_dummy)
coef(m_dummy) # Включена только одна фиктивная переменная
```

Количество фиктивных переменных, необходимых для учета в модели категориальной переменной, должно быть на единицу меньше числа категорий, чтобы предотвратить линейную зависимость. Вспомним, что базовой категорией будет та, для которой в модель не включена фиктивная переменная. Коэффициенты для включенных в модель фиктивных переменных показывают отличия соответствующих групп от базовой.

В R для моделирования дискретных переменных лучше использовать факторы, при этом фиктивные переменные создаются автоматически и линейной зависимости между ними не может возникнуть в принципе.


# Отбор предикторов в модель


## Полная модель

Попробуем построить **полную модель** (*full model*), включающую всю доступную информацию, т.е. все количественные переменные в наборе данных, а также тип машины и страну производителя.

```{r Полная модель}

m_full <- lm(mpg ~ ., data = dplyr::select(cars, country:mpg)) # Символ . обозначает все переменные в таблице

summary(m_full)

vif(m_full) %>% round(1)

```

У предикторов большие коэффициенты роста дисперсии (VIF). Некоторые угловые коэффициенты незначимы из-за больших стандартных ошибок.
Причина в том, что многие объясняющие переменные взаимосвязаны и несут дублирующуюся информацию. Необходимо упростить модель.


## Критерии для выбора лучшей модели

До сих пор, мы ориентировались на коэффициент детерминации $R^2$ для сравнения моделей. Однако у этого показателя есть ограничения. Можно показать, что при усложнении модели путем включения дополнительных переменных величина $R^2$ всегда увеличивается, даже если новые переменные не несут дополнительной информации. Таким образом, с помощью $R^2$ можно сравнивать только модели, у которых одинаковое число предикторов.

На практике для выбора моделей используются другие показатели, которые на имеют этой проблемы:

 - Скорректированный $R^2$ (Adjusted $R^2$)
 - Информационный критерий Акаике (Akaike Information Criterion, AIC)
 - Модифицированный информационный критерий Акаике, (Corrected AIC, AICc)
 - Байесовский информационный критерий Шварца (Bayesian Information Criterion, BIC)
 - Статистика [Mallow's Cp](https://en.wikipedia.org/wiki/Mallows's_Cp)
 
Все эти показатели используют один и тот же принцип - более сложные модели "штрафуют" за увеличение количества предикторов. К ошибке модели добавляется штраф, который тем больше, чем больше предикторов включено в модель. Рассмотрим это на примере скорректированного $R^2$ и информационных критериев.
 
 
 **Скорректированный  $R^2$** вычисляется по формуле:
 
 $$ R^2_{adj} = 1 - (1-R^2) \cdot \frac{n-1}{n-k-1}$$
 
Чем больше число предикторов $k$, тем больше дробь, и тем больше произведение этой дроби на долю необъясненной дисперсии $(1-R^2)$. Таким образом, доля объясненной дисперсии занижается.


**Информационный критерий Акаике (AIC)** вычисляется на основе дисперсии остатков модели (остаточной суммы квадратов), но он также содержит штрафной компонент, зависящий от числа предикторов: 

$$ AIC = n \ln (RSS/n) + 2(k+2) $$

Здесь штраф включен во второе слагаемое: $k + 2$ - это общее количество оцениваемых по выборке параметров: $k$ угловых коэффициентов, свободный член и стандартная ошибка модели. У AIC нет верхнего и нижнего предела. Меньшее значение AIC соответствует лучшей модели.

Если число наблюдений $n$ невелико, то при сравнении моделей по AIC получаются слишком сложные модели с большим числом предикторов. Поэтому была разработана модификация - **скорректированный AIC (Corrected AIC, AICc)**, в который дополнительно включен штраф за сложность, который линейно уменьшается при увеличении размера выборки:

$$ AICc = AIC + \frac{2 (k+2) (k+3)}{n - k - 3} $$


**Байесовский информационный критерий Шварца (BIC)** вычисляется по аналогичному принципу, однако в нем используется другой штраф:

$$ BIC = n \ln (RSS/n) + (k+2) \cdot \ln n $$

В BIC штраф за увеличении сложности выше, поэтому при использовании этого критерия отбора предпочтение отдается моделям с меньшим числом предикторов.

```{r Сравнение штрафов в AIC и BIC, echo=FALSE}

f_AIC <- function(k) { 
  2 * (k + 2)
}

f_AICc <- function(k, n) {
  f_AIC(k) + 2 * (k + 2) * (k + 3) / (n - k - 3)
}

f_BIC <- function(k, n) {
  (k + 2) * log(n)
}


ggplot(data = tibble(k = seq(0, 30)), 
       mapping = aes(x = k)) +
  stat_function(mapping = aes(colour = "AIC"), 
                fun = f_AIC) +
  
  stat_function(mapping = aes(colour = "AICc, n = 50"), 
                fun = f_AICc, args = list(n = 50)) +
  
  stat_function(mapping = aes(colour = "AICc, n = 500"), 
                fun = f_AICc, args = list(n = 500), 
                linetype = 'dashed' ) +


  stat_function(mapping = aes(colour = "BIC, n = 50"), 
                fun = f_BIC, args = list(n = 50)) +
  
  stat_function(mapping = aes(colour = "BIC, n = 500"), 
                fun = f_BIC, args = list(n = 500), 
                linetype = 'dashed' ) +
  
  scale_colour_manual(values = c("gold", "red", "darkred", "lightskyblue", "blue")) +
  
  labs(title = "Сравнение штрафов AIC, AICc и BIC",
       x = "Число параметров k",
       y = "Штраф")

```



## Метод полного перебора

Для небольших наборов данных можно применить для выбора наилучшей модели (по заданному критерию) метод полного перебора. Однако число моделей, которые необходимо рассмотреть экспоненциально возрастает, поэтому данный подход невозможно масштабировать.

В R есть функция `leaps::regsubsets()`, которая решает задачу выбора наилучшей модели при заданном числе предикторов (*best subsets regression*). Эта функция сравнивает модели на основе скорректированного $R^2$.

```{r Выбор лучшей модели полным перебором}
best_models <- leaps::regsubsets(mpg ~ ., # рассматриваем все возможные предикторы
                                 nbest = 2,  # две лучших модели для каждого числа предикторов
                                 nvmax = 10, # максимально 10 переменных в модели, по умолчанию - 8
                                 data = dplyr::select(cars, country:mpg))

# Сводка по моделям
summary(best_models) 

```


Можно визуализировать результаты сравнения моделей средствами базовой графики R.

```{r Визуализация результатов сравнения моделей, fig.height = 7}

# Создаем палитру менее депрессивных цветов
# См. RColorBrewer::display.brewer.all()
mycolors <- rev(RColorBrewer::brewer.pal(n = 5, name = 'Greens')) 

# Визуализация моделей
plot(best_models, scale = "adjr2", col = mycolors,
     main = "Сравнение лучших моделей по скорректированному R^2")

```

На этом графике модели упорядочены по убыванию скорректированного $R^2$. Каждой модели соответствует одна строка в матрице. Ячейки, закрашенные цветом - это выбранные предикторы, белые ячейки - не включенные в модель переменные. 

Вверху находится наилучшая по этому критерию модель, в которую включены все предикторы, кроме длины и ширины машины.

Если выбрать другой критерий ранжирования, то лучшей окажется другая модель. Например, при выборе BIC, в котором используется больший штраф за усложнение модели, результат получится другой:

```{r Лучшая модель по BIC, fig.height = 7}
# Визуализация моделей
plot(best_models, scale = "bic", col = mycolors, 
     main = "Сравнение лучших моделей по BIC")

```

Здесь в лучшей модели (с наименьшим BIC) оказалось гораздо меньше переменных - 5 вместо 8.


**Ограничения метода**

Помимо неоднозначности результатов в зависимости от выбора критерия оценки моделей, и больших вычислительных затрат, у метода полного перебора есть еще один недостаток: если зависимость между переменными нелинейная и нужно преобразование данных, то такое преобразование автоматически выполняться не будет. Ответственность за выяснение этой необходимости и подготовка данных лежит на аналитике. Также не рассматриваются взаимодействия предикторов.


## Пошаговая регрессия

**Пошаговая регрессия** (*stepwise regression*) - это эвристический алгоритм для отбора переменных в регрессионную модель, который можно применять даже когда количество возможных предикторов велико и полный перебор невозможен. Алгоритм работает итеративно. На каждом шаге принимается решение, какую переменную лучше всего включить в модель, или исключить из нее, чтобы повысить точность. Существуют три модификации алгоритма пошаговой регрессии:

- Пошаговое исключение переменных (*backward stepwise*) - путь упрощения сложной модели
- Пошаговое включение переменных (*forward stepwise*) - путь усложнения простой модели
- Пошаговый перебор (*Stepwise stepwise*) - путь последовательного изменения модели, при этом на каждом шаге она может быть усложнена или упрощена


Метод пошагового исключения начинает с полной модели и последовательно исключает наихудший предиктор на каждом шаге, до тех пор пока не окажется, что дальнейшее упрощение модели ухудшает ее качество. 

Метод пошагового включения действует в обратном напралении: работа начинается с нулевой модели без предикторов, и на каждом шаге определяется, какую из переменных лучше всего включить в модель как предиктор.

Метод пошагового отбора после включения на очередном шаге новой переменной в модель, может исключать из модели добавленные ранее переменные, если это улучшит точность. Такая необходимость возникает в том случае, если переменные коррелированы и добавленная на более позднем шаге переменная содержит ту же информацию, что и ранее добавленная.

Для оценки качества модели на каждом шаге используется какой-либо "штрафной" критерий - например, AIC или скорректированный $R^2$.

В R алгоритм пошаговой регрессии реализован функцией `MASS:stepAIC()`. 

Рассмотрим использование алгоритма пошаговой регрессии на примерах.


### Пошаговое исключение (backward stepwise)

Метод пошагового исключения основан на упрощении первоначальной модели. Чтобы задать эту модель, можно использовать формулу, или ранее полученную с помощью `lm()` модель. 

```{r Backward Stepwise}

m_backward <- stepAIC(m_full, direction = "backward")

```

При необходимости, можно отключить вывод результатов на каждом шаге, задав параметр `trace = F`. Можно менять весовой коэффициент для штрафа (параметр `k`). По умолчанию он равен 2 (AIC), если задать величину log(n), где `n` - число наблюдений в выборке, то получится BIC.


Для использования пошагового включения или перебора необходимо задать направление поиска, указав наиболее простую и наиболее полную модели, которые надо рассмотреть, в параметре `scope = `. Здесь мы используем в качестве границ нулевую и полную модели.

### Пошаговое включение (forward stepwise)

```{r Forward Stepwise}

# Нулевая модель
m_null <- lm(mpg ~ 1, data = dplyr::select(cars, country:mpg))

# Прямое включение
m_forward <- stepAIC(m_null, 
                     scope = list(lower = m_null, upper = m_full),
                     direction = 'forward')

```


### Пошаговый перебор (stepwise stepwise)

```{r Stepwise Stepwise}
m_stepwise <- stepAIC(m_null, 
                      scope = list(lower = m_null, upper = m_full), 
                      direction = 'both')
```

Для сравнения, подберем наилучшую модель по критерию BIC, используя метод пошагового перебора.

```{r Stepwise Stepwise c BIC}
m_stepwise_BIC <- stepAIC(m_null, 
                      scope = list(lower = m_null, upper = m_full), 
                      direction = 'both',
                      k = log(nrow(cars))) # в BIC штраф = (k + 2) * log(n)

```



Сравним модели, полученные пошаговым отбором, и лучшую модель, полученную полным перебором вариантов.


```{r Сравнение моделей при пошаговом отборе}

m_best <- lm(mpg ~ type + horsepow + curb_wgt + fuel_cap, data = cars)

memisc::mtable(m_backward, m_forward, m_stepwise, m_stepwise_BIC, m_best)
```



**Ограничения метода**

В данном случае, все три алгоритма последовательного перебора привели к одинаковому результату (при использовании AIC). При использовании BIC получена модель с меньшим числом предикторов. Эта модель совпала с лучшей моделью, найденной полным перебором. Однако в общем случае такой результат не гарантируется. Алгоритмы пошагового отбора - **"жадные"**, они принимают локально оптимальное решение на каждом шаге (выбирают модель с наилучшим приростом критерия) и не могут вернуться назад, если поиск зайдет в тупик. Поэтому гарантий нахождения глобального оптимума (наилучшей возможной модели) нет. 

Кроме того, как и в методе полного перебора, ответственность за нелинейные преобразования данных или учет взаимодействия предикторов лежит на аналитике. Автоматические методы их учесть не могут.

## Регуляризация

Как мы видели, в полной модели увеличивается погрешность оценки коэффициентов, некоторые из них становятся незначимыми из-за мультиколлинеарности. Помимо проблем статистического вывода, более сложные модели в большей степени подвержены переобучению. При этом обученная на выборке модель "запоминает" не только закономерности, присутствующие в этих данных, но и случайный шум. При прогнозировании такая модель дает худший результат, чем более простая модель. Поэтому основная идея рассмотренных нами методов состоит в том, чтобы искусственно ограничить количество переменных в модели, чтобы она не становилась слишком сложной. 

Однако существует и другой подход к решению проблемы чрезмерного усложнения модели. Вместо того, чтобы ограничивать количество предикторов, можно ограничить величину самих коэффициентов модели - $b_j$. Такой подход, основанный на ограничении коэффициентов, называется **регуляризацией** (*regularization*). При оценке параметров модель "штрафуют" за слишком большую их величину.

Существуют 2 модификации метода регуляризации, в которых используются разные формулы для штрафа.

- В методе **гребневой регрессии** (*ridge regression*) в качестве штрафа используется сумма квадратов коэффициентов (L2-норма):

$$ RSS(b) + \lambda \sum_{j=1} ^k {b_j^2} \to min$$


- В методе **лассо** (*Least absolute shrinkage and selection operator, Lasso*) используется сумма модулей коэффициентов (L1-норма):

$$ RSS(b) + \lambda \sum_{j=1} ^k |b_j| \to min$$
Свое название метод получил благодаря тому, что при использовании модуля в качестве штрафной функции оптимальные значения коэффициентов получаются часто нулевыми. Таким образом, модель как бы "отлавливает" лучшие предикторы из "табуна" всех возможных.

В обеих штрафных функциях $\lambda$ - это параметр, определяющий вес штрафа, т.е. степень регуляризации. Этот параметр подбирается экспериментально, путем сравнения точности полученных моделей на тестовой выборке.

В R оба метода реализованы в пакете `glmnet`. При этом данный пакет реализует сочетание методов лассо и ридж-регрессии, названное авторами методом **эластичной сети** (*elastic net*). Данный подход включает обе штрафных функции, позволяя задавать вес для каждой из них с помощью параметра $\alpha$:

$$ RSS(b) + \lambda \left(\alpha\sum_{j=1} ^k |b_j| + (1 - \alpha) \sum_{j=1} ^k {b_j^2} \right) \to min$$

### Подготовка данных

К сожалению, данный пакет не работает с датафреймами, поэтому необходимо позаботиться о том, чтобы привести данные к необходимому для этого пакету виду: целевая переменная - в отдельном векторе, предикторы - в матрице. Категориальные переменные необходимо вручную преобразовать в наборы фиктивных переменных.

```{r Подготовка данных для glmnet, warning = F, message = F}
y <- cars$mpg

X <- cars %>%
  mutate(truck = ifelse(type == 'Truck', 1, 0), 
         europe = ifelse(country == 'Europe', 1, 0)) %>%
  select(-(manufact:type), -mpg) %>%
  as.matrix()

head(X, 4) 
```

### Ридж-регрессия в glmnet

Построим модель ридж-регрессии с помощью функций пакета `glmnet`. Параметр $\alpha = 0$, параметр $\lambda$ будет выбран автоматически.

```{r Ридж-регрессия в glmnet - подбор параметра}

# Подбор параметра lambda
set.seed(12345)
cv_ridge <- cv.glmnet(X, y, alpha = 0, parallel = T)


# Зависимость ошибки модели на тестовой выборке от параметра lambda
plot(cv_ridge, xvar = "lambda")

# Зависимость коэффициентов от параметра lambda
plot(cv_ridge$glmnet.fit, xvar = "lambda", label = T)

```

При увеличении параметра $\lambda$ коэффициенты модели стремятся к нулю. 

```{r Ридж регрессия - коэффициенты лучшей модели}


# Лучшее значение коэффициента lambda
cat("Минимальная ошибка при lambda =", cv_ridge$lambda.min, "\n\n")


# Коэффициенты модели с минимальной ошибкой на тестовой выборке
coef(cv_ridge, s = c(0, cv_ridge$lambda.min))

```

В таблице коэффициентов приведены значения коэффициентов модели без регуляризации (в столбце 1) и при оптимальном значении $\lambda$ (в столбце 2).

Сделаем прогноз для автомобиля европейского производства со следующими параметрами:

- цена: 40 тысяч $, 
- объем двигателя: 4 литра,
- мощность: 300 л.с.,
- колесная база: 130 дюймов,
- ширина: 70 дюймов,
- длина: 200 дюймов,
- вес: 4 тысячи фунтов,
- емкость бака: 20 галлонов.


Значения предикторов должны находиться в матрице. В нашем случае матрица будет иметь всего одну строку, т.к. автомобиль один. Возможные способы задания матрицы описаны в справке: `?matrix`.

```{r Новые данные}

new_car <- tibble( # таблица с одной строкой
   price = 40,
   engine_s = 4,
   horsepow = 300,
   wheelbas = 130,
   width = 70,
   length = 200,
   curb_wgt = 4,
   fuel_cap = 20,
   truck = 0,
   europe = 1) %>% 
  as.matrix() # преобразовали  в матрицу 

new_car 
```


```{r Прогноз по ридж-модели}

# Прогноз
pred <- predict(cv_ridge, newx = new_car, s = c(0, cv_ridge$lambda.min))

# Вывод результата
colnames(pred) <- c("Без регуляризации", "С регуляризацией")
pred
```

В данном случае результаты прогнозирования не отличаются, т.к. коэффициенты моделей близкие.

### Лассо-регрессия в glmnet

Построим модель лассо-регрессии. Параметр $\alpha = 1$, параметр $\lambda$ будет выбран автоматически.

```{r Лассо-регрессия в glmnet - подбор параметра}

# Подбор параметра lambda
set.seed(12345)
cv_lasso <- cv.glmnet(X, y, alpha = 1, parallel = T)


# Зависимость ошибки модели на тестовой выборке от параметра lambda
plot(cv_lasso, xvar = "lambda")


# Зависимость коэффициентов от параметра lambda
plot(cv_lasso$glmnet.fit, xvar = "lambda", label = T)

```

При увеличении параметра $\lambda$ коэффициенты модели стремятся к нулю. В таблице коэффициентов приведены значения коэффициентов модели без регуляризации и при оптимальном значении $\lambda$.

```{r Лассо регрессия - коэффициенты лучшей модели}

# Лучшее значение коэффициента lambda
cat("Минимальная ошибка при lambda =", cv_lasso$lambda.min, "\n\n")



# Коэффициенты модели с минимальной ошибкой на тестовой выборке
coef(cv_lasso, s = c(0, cv_lasso$lambda.min))

```

Часть переменных (`price`, `wheelbas`, `width`, `length`) получили нулевые угловые коэффициенты и, таким образом, были исключены из модели.

Сделаем прогноз:

```{r Прогноз по модели лассо}

# Прогноз
pred <- predict(cv_lasso, newx = new_car, s = c(0, cv_lasso$lambda.min))

# Вывод результата
colnames(pred) <- c("Без регуляризации", "С регуляризацией")

pred

```

В данном случае результаты прогнозирование немного отличаются, т.к. в результате лассо-регрессии часть переменных были исключены из модели.

Другие примеры использования пакета glmnet можно посмотреть здесь:

 -  **Trevor Hastie, Junyang Qian** [glmnet Vignette](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html)
 -  **Ricardo Carvalho** [How to use Ridge Regression and Lasso in R](http://ricardoscr.github.io/how-to-use-ridge-and-lasso-in-r.html)
 - Справка по `glmnet`

# Стандартизация и центрирование переменных
## Какие предикторы сильнее всего влияют на целевую переменную?

Рассмотрим лучшую модель, найденную с использованием BIC.

```{r Сводка по лучшей модели}
tidy(m_stepwise_BIC)

```

В модели 4 переменных. Ответить на вопрос о том, какие из них сильнее всего влияют на топливную эффективность - сложно. Коэффициенты в модели зависят от масштаба включенных в модель переменных. Например, вес машин измеряется в тысячах фунтов и масштаб значений в таблице данных - единицы, а мощность машин - в лошадиных силах, масштаб - сотни. Поэтому и коэффициенты для этих переменных не сопоставимы между собой.

```{r Диапазон значений различных переменных в модели}
cars_dummy %>% 
  select(curb_wgt, fuel_cap, truck, horsepow) %>%
  map(range) %>% 
  as_tibble()
```


Чтобы сопоставить коэффициенты, необходимо привести все переменные к одному масштабу. Одним из способов решения этой задачи является **стандартизация**, при которой каждая переменная заменяется на свой *z-стандарт* (z-score):

$$ z = \frac{x - \bar{x}}{S_x} $$

Значения исходной переменной $x$ заменяются на отклонения этих значений от среднего, выраженные в количестве стандартных отклонений. Например, $z=-1.5$ означает, что соответствующее значение $x$ было ниже среднего на 1.5 стандартных отклонения.

Данный способ лучше всего работает, когда распределения переменных близки к нормальному. В результате все преобразованные переменные имеют одно и то же стандартное нормальное распределение, то есть у них одинаковый масштаб. Асимметрия распределений и наличие выбросов могут существенно исказить картину. 

Для стандартизации переменных в R используется функция `scale()`.

Масштабировать можно только количественные переменные. Для категориальных придется вручную создавать фиктивные переменные.

```{r Стандартизация данных}

cars_dummy_z <- cars_dummy  %>%
  mutate_if(is.numeric, scale)

cars_dummy_z_tall <- cars_dummy_z %>% 
  select(curb_wgt, fuel_cap, truck, horsepow, mpg) %>% 
  gather(key = "Переменная", value = "Значение", curb_wgt:mpg)

# Описательная статистика
cars_dummy_z_tall %>% 
  group_by(`Переменная`) %>%
  summarize(`Среднее` = round(mean(`Значение`), 3), 
            `Стандартное отклонение` = sd(`Значение`))

# Визуализация распределений
ggplot(data = cars_dummy_z_tall) +
  geom_density(aes(x = `Значение`, colour = `Переменная`)) +
  labs(title = "Распределения переменных после стандартизации",
       y = "Плотность")

```


Рассмотрим модель с теми же предикторами, но построенную на стандартизованных данных о выходной переменной и предикторах.


```{r Модель со стандартизованными данными}

m_stepwise_BIC_z <- lm(mpg ~ curb_wgt + fuel_cap + truck + horsepow, data = cars_dummy_z) 

tidy(m_stepwise_BIC_z) %>%
  mutate_if(is.numeric, round, digits = 4)

```

Коэффициенты полученной модели называются **стандартизованными коэффициентами** (*standardized coefficients *). Они позволяют судить о том, какой из предикторов сильнее влияет на выходную переменную.
Например, мы видим, что увеличение мощности  на одно стандартное отклонение снижает пробег на галлоне на 0.337 стандартных отклонения, а увеличение веса машины - только на 0.259 стандартных отклонения, поэтому мощность сильнее влияет на целевую переменную, чем вес. Однако нужно учитывать, что оцененные по выборке коэффициенты модели имеют погрешность, и в данном случае правильнее сделать вывод о том, что степень влияния этих факторов довольно близкая.

Стандартизацию не вполне корректно применять к фиктивным переменным, т.к. их распределение - дискретно и включает лишь 2 значения - 0 и 1, а стандартизация ориентирована на нормальное распределение. В связи с этим, существует другой подход, в котором стандартизуются только количественные предикторы. Чтобы сделать масштаб преобразованных переменных сопоставимым с диапазоном для фиктивных переменных, было предложено при стандартизации делить отклонение не на одно, а на два стандартных отклонения. Бинарные переменные при этом преобразовывать не надо. Это выгодно, т.к. данный подход позволяет не создавать фиктивные переменные вручную, а использовать факторы.

См. Andrew Gelman, [Standardizing regression inputs by dividing by two standard deviations](http://andrewgelman.com/2006/06/21/standardizing_r/)



```{r Стандартизация на 2 стандартных отклонения}

scale2 <- function(x) {
  scale(x)/2
}

cars_2z <- cars  %>%
  mutate_if(is.numeric, scale2)

```

Коэффициенты для линейной модели при стандартизации на 2 стандартных отклонения:

```{r Коэффициенты модели при стандартизации на 2 стандартных отклонения}

m_stepwise_BIC_2z <- lm(mpg ~ curb_wgt + fuel_cap + type + horsepow, data = cars_2z) 

tidy(m_stepwise_BIC_2z) %>%
  mutate_if(is.numeric, round, digits = 4)

```

Качественно картина получилась такой же. Самый важный предиктор - `horsepow`. Изменение типа машины снижает пробег на галлоне на 0.337 стандартных отклонения - второй по важности предиктор.

## Автомобиль нулевого веса?

Вернемся к модели с непреобразованными данными

```{r Коэффициенты исходной модели}
tidy(m_stepwise_BIC) %>% select(term:estimate)
```

Угловые коэффициенты для переменных имеют понятную интерпретацию: например, увеличение веса машины на 1 тысячу фунтов снижает пробег, в среднем, на 1.75 мили. Однако как в данном случае интерпретировать свободный член $b_0$? С точки зрения модели, это пробег легковой машины нулевого веса, мощности и без топливного бака. Очевидно, что с практической точки зрения такой результат бесполезен. 
Чтобы сделать полезной интерпретацию свободного члена в тех случаях, когда нулевые значения предикторов не имеют смысла, используется прием, основанный на **центрировании** (*centering*) таких предикторов. При центрировании значения переменной заменяются на отклонение от среднего значения:

$$ x_c = x - \bar{x} $$

После центрирования среднее значение переменной становится равным нулю, а сами значения представляют собой отклонения от среднего, "типичного"  значения.


В R центрирование можно провести с помощью той же самой функции `scale()` с параметром `scale = F`.

```{r Центрирование предикторов}

cars_c <- cars %>% 
  mutate_at(c("curb_wgt", "fuel_cap", "horsepow"), # к каким переменным применить центрирование
            scale, # применяемая функция
            scale = F) %>% # отключить масштабирование путем деления на стандартное отклонение
  mutate(horsepow = horsepow / 100) # мощность теперь измеряется в сотнях л.с.


cars_c %>% head(3)

```

Построим модель с центрированными предикторами.

```{r Модель с центрированными предикторами}
m_stepwise_BIC_c <- lm(mpg ~ curb_wgt + fuel_cap + type + horsepow, data = cars_c) 

mtable(m_stepwise_BIC, m_stepwise_BIC_c)

```

Угловые коэффициенты для предикторов не изменились (кроме horsepow, который увеличился в 100 раз, т.к. теперь отражает изменение пробега при увеличении мощности на 100 л.с.). Свободный член теперь имеет полезную интерпретацию: *"Легковая машина среднего веса и мощности, со средним по размеру топливным баком, может проехать на 1 галлоне топлива 24.6 мили."*

Интерпретация угловых коэффициентов не изменилась. Например, для веса машины по-прежнему верно, что его увеличение на 1 тысячу фунтов снижает пробег на 1.75 мили (при прочих равных характеристиках).

# Диагностика остатков модели

Как и в случае линейной регрессии, модель множественной регрессии основана на ряде допущений о свойствах случайного компонента модели, т.е. остатков. См. [первую часть](mlr-modeling.Rmd) лекции.

Для диагностики остатков множественной регрессии можно воспользоваться теми же инструментами, что и для линейной регрессии.

## Остатки исходной модели

```{r Остатки исходной модели множественной регресии}
autoplot(m_stepwise_BIC)
```

На графике остатков в зависимости от предсказанных значений можно видеть небольшое искривление, которое является следствием нелинейности зависимости. Поэтому можно рассмотреть вопрос о необходимости преобразования целевой переменной, или включения в модель нелинейных членов. Отклонения от нормальности заметны только в хвостах наблюдений, но число таких наблюдений невелико. Рост дисперсии при увеличении целевой переменной не наблюдается.

## Выбросы и влиятельные наблюдения{#residuals_stepwise}
Рассмотрим выбросы и влиятельные наблюдения более подробно.

```{r Выбросы и влиятельные наблюдения}
fit_stepwise_BIC <- m_stepwise_BIC %>% 
  fortify() %>% # сохранили остатки, предсказания и показатели влиятельности в набор
  merge(., cars) # добавили исходные данные (merge удаляет дублирующиеся при слиянии столбцы)

ggplot(fit_stepwise_BIC, aes(.fitted, mpg)) +
  geom_point(aes(size = .cooksd, colour = .hat)) +
  geom_line(aes(.fitted, .fitted), 
            colour = "red", linetype = "dashed") +
  scale_colour_gradient(low = "black", high = "red") +
  ggrepel::geom_label_repel(
    data = filter(fit_stepwise_BIC, .cooksd > 4/nrow(cars)),
    mapping = aes(label = model), alpha = 0.25) +
  labs(title = "Влиятельные наблюдения",
       x = "Прогноз mpg", y = "Факт mpg", 
       colour = "Леверидж", size = "Расстояние Кука")
 
```

```{r Зависимость остатков от предсказанных значений}

ggplot(fit_stepwise_BIC, aes(.fitted, .resid)) +
  geom_point(aes(size = .cooksd, colour = .hat)) +
  geom_hline(yintercept = 0,
            colour = "red", linetype = "dashed") +
  scale_colour_gradient(low = "black", high = "red") +
  ggrepel::geom_label_repel(
    data = filter(fit_stepwise_BIC, .cooksd > 4/nrow(cars)),
    mapping = aes(label = model), alpha = 0.25) +
  labs(title = "Зависимость остатков от предсказанных значений",
       x = "Прогноз mpg", y = "Остаток", 
       colour = "Леверидж", size = "Расстояние Кука")
 
```


Наибольший остаток имеет модель [Chevrolet Metro](https://wiki.zr.ru/Chevrolet_Metro). Это самая маленькая и экономичная из американских машин. 


## Нелинейное преобразование целевой переменной

Один из способов устранения нелинейности - степенное преобразование целевой переменной. Подходящую степень преобразования можно определить с помощью функции `car::powerTransform()`. Эта функция использует в качестве критерия выбора степень близости распределения остатков модели после преобразования к нормальному распределению.

```{r Определение степени преобразования}
powerTransform(m_stepwise_BIC)
```

Полученную степень лучше округлить. Значение довольно близко к нулю, который соответствует использованию логарифмического преобразования.

Попробуем оценить модель с преобразованной целевой переменной.

```{r Подгонка модели с логарифмом}

m_log <- lm(log10(mpg)  ~ curb_wgt + fuel_cap + type + horsepow, data = cars)

tidy(m_log) %>% mutate_if(is.numeric, round, digits = 3)

```



```{r Остатки модели с логарифмом}
autoplot(m_log)
```

Ситуация улучшилась незначительно. Появилось искривление на графике для оценки гетероскедастичности. В данном случае предпочтение стоит отдать более простой модели без логарифма.

## Добавление нелинейных предикторов в модель

Для выявления нелинейных зависимостей от объясняющих переменных и пропущенных предикторов можно применять метод графического анализа остатков, строя диаграммы рассеяния для остатков в зависимости от других переменных в наборе данных.

```{r Зависимость остатков исходной модели, fig.height=7}
fit_cars_stepwise <- cars %>% 
  add_residuals(m_stepwise_BIC) %>%
  gather(key = "Переменная", value = "Значение", price:fuel_cap)

ggplot(fit_cars_stepwise, aes(x = `Значение`, y = `resid`)) +
  geom_point() + 
  geom_smooth(se = F) +
  facet_wrap(~ `Переменная`, scales = "free_x", ncol = 2) +
  labs(title = "Остатки исходной модели в зависимости от других переменных",
       y = "Остаток", x = NULL)
```

Выраженной зависимости остатков от не включенных в модель переменных не наблюдается (есть сомнения только насчет объема двигателя). Видна нелинейность зависимости пробега от веса. Поэтому можно попытаться включить в модель члены более высоких порядков для веса.


```{r Подгонка модели со степенями веса}

m_poly <- lm(mpg  ~ curb_wgt + I(curb_wgt^2) + I(curb_wgt^3) + I(curb_wgt^4) + 
            fuel_cap + type + horsepow,
             data = cars)

mtable(m_stepwise_BIC, m_poly)


```

Показатели немного улучшились, коэффициенты для веса и его степеней значимы.

Мы получили довольно сложную модель:

$$ \widehat{mpg} = 301 - 284 curb\_wgt + 111 curb\_wgt^2 - 19.0 curb\_wgt^3 + 1.192 curb\_wgt^4 -\\
- 0.240 fuel\_cap - 3.15 (type = Truck) - 0.023 horsepow $$

Сравним остатки.

```{r Остатки модели со степенями веса}
autoplot(m_poly)
```

Ситуация улучшилась, нелинейность практически не заметна.

## Остатки финальной модели
Зависимость остатков новой модели от остальных переменных.

```{r Остатки полиномиальной модели в зависимости от остальных переменных, fig.height=7}
fit_cars_poly <- cars %>% 
  add_residuals(m_poly) %>%
  gather(key = "Переменная", value = "Значение", price:fuel_cap)

ggplot(fit_cars_poly, aes(x = `Значение`, y = `resid`)) +
  geom_point() + 
  geom_smooth(se = F) +
  facet_wrap(~ `Переменная`, scales = "free_x", ncol = 2) +
  labs(title = "Остатки полиномиальной модели в зависимости от других переменных",
       y = "Остаток", x = NULL)
```

Зависимости остатков от других переменных исчезли. Таким образом информация, доступная в этом наборе данных, была использована полностью.

В заключение сравним предсказания модели с фактом. 

```{r Выбросы и влиятельные наблюдения полиномиальной модели}
fit_cars_poly <- m_poly %>% 
  fortify() %>% # сохранили остатки, предсказания и показатели влиятельности в набор
  merge(., cars) # добавили исходные данные (merge удаляет дублирующиеся при слиянии столбцы)

ggplot(fit_cars_poly, aes(.fitted, mpg)) +
  geom_point(aes(size = .cooksd, colour = .hat)) +
  geom_line(aes(.fitted, .fitted), 
            colour = "red", linetype = "dashed") +
  scale_colour_gradient(low = "black", high = "red") +
  ggrepel::geom_label_repel(
    data = filter(fit_cars_poly, .cooksd > 4/nrow(cars)),
    mapping = aes(label = model), alpha = 0.25) +
  labs(title = "Влиятельные наблюдения",
       x = "Прогноз mpg", y = "Факт mpg", 
       colour = "Леверидж", size = "Расстояние Кука")
 
```

В сравнении с результатом для предыдущей модели (см. [Выбросы и влиятельные наблюдения](#residuals_stepwise)) искривление облака точек практически не заметно, точки симметрично расположены относительно прямой "идеального результата". Отклонения прогноза и факта существенно уменьшились. 


```{r Зависимость остатков полиномиальной модели от предсказанных значений}

ggplot(fit_cars_poly, aes(.fitted, .resid)) +
  geom_point(aes(size = .cooksd, colour = .hat)) +
  geom_hline(yintercept = 0,
            colour = "red", linetype = "dashed") +
  scale_colour_gradient(low = "black", high = "red") +
  ggrepel::geom_label_repel(
    data = filter(fit_cars_poly, .cooksd > 4/nrow(cars)),
    mapping = aes(label = model), alpha = 0.25) +
  labs(title = "Зависимость остатков от предсказанных значений",
       x = "Прогноз mpg", y = "Остаток", 
       colour = "Леверидж", size = "Расстояние Кука")
 
```


Наибольшими остатками и расстоянием Кука выделяются два типа машин - экономичные машины (Chevrolet Metro, Hyundai Accent, Mitsubishi Mirage), крупные легковые автомобили (Dodge Ram Wagon, Cadillac Escalade, Lincoln Navigator, Honda Odyssey, Toyota RAV4, Jeep Wrangler) и спортивные машины (Dodge Viper, Plymouth Prowler). Возможно, в этой области имеется потенциал для улучшения модели путем включения типа машины в качестве объясняющей переменной.

